{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "veubc7lbxdlp"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/UMass/\"685 PROJECT\"/\"LSR-VQ\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "-43ssCagua2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bd751aa-b15d-4b1b-fa9a-727efa849665"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/UMass/685 PROJECT/LSR-VQ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Dataset (Don't run!!! Data is already downloaded)"
      ],
      "metadata": {
        "id": "veubc7lbxdlp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "h-u-Rg2PuAc1",
        "outputId": "d3cf5e27-3b27-4926-a74b-e3fc56afcec6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-31 19:45:49--  https://msmarco.z22.web.core.windows.net/msmarcoranking/collection.tar.gz\n",
            "Resolving msmarco.z22.web.core.windows.net (msmarco.z22.web.core.windows.net)... 20.150.34.1\n",
            "Connecting to msmarco.z22.web.core.windows.net (msmarco.z22.web.core.windows.net)|20.150.34.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1035009698 (987M) [application/octet-stream]\n",
            "Saving to: ‘data/collection.tar.gz’\n",
            "\n",
            "collection.tar.gz   100%[===================>] 987.06M  50.5MB/s    in 20s     \n",
            "\n",
            "2025-03-31 19:46:09 (49.1 MB/s) - ‘data/collection.tar.gz’ saved [1035009698/1035009698]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# # Download collection (pId -> passage text)\n",
        "# !wget -P data/raw/ https://msmarco.z22.web.core.windows.net/msmarcoranking/collection.tar.gz\n",
        "\n",
        "# # Download queries (qId -> query text)\n",
        "# !wget -P data/raw/ https://msmarco.z22.web.core.windows.net/msmarcoranking/queries.tar.gz\n",
        "\n",
        "# # Download qRels Dev and Train\n",
        "# !wget -P data/raw/ https://msmarco.z22.web.core.windows.net/msmarcoranking/qrels.dev.tsv\n",
        "# !wget -P data/raw/ https://msmarco.z22.web.core.windows.net/msmarcoranking/qrels.train.tsv\n",
        "\n",
        "# # Download training data - qId positive_pId and negative_pId\n",
        "# !wget -P data/raw/ https://msmarco.z22.web.core.windows.net/msmarcoranking/qidpidtriples.train.full.2.tsv.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # unzip files\n",
        "# !tar -xf \"data/raw/queries.tar.gz\"\n",
        "# !tar -xf \"data/raw/collection.tar.gz\"\n",
        "# !tar -xf \"data/raw/qidpidtriples.train.full.2.tsv.gz\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxkrklhEyVdm",
        "outputId": "3e7eac4b-9ba9-4692-9e69-58017a15f3a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mraw\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "gMYgXkrF1mZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv"
      ],
      "metadata": {
        "id": "TuqmlacdxLBH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the tsv file as a dictionary\n",
        "def open_file(file_path, keys = [0, 1]):\n",
        "    with open(file_path, mode = \"r\", encoding = \"utf-8\") as file:\n",
        "        reader = csv.reader(file, delimiter= \"\\t\")\n",
        "        data = {row[keys[0]]: row[keys[1]] for row in reader}\n",
        "\n",
        "    return data\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "def load_and_preprocess_dataset():\n",
        "    # Load all passages\n",
        "    passages = open_file(\"data/collection.tsv\")\n",
        "\n",
        "    # Load all train, dev & eval queries\n",
        "    queries_train = open_file(\"data/queries.train.tsv\")\n",
        "    queries_dev = open_file(\"data/queries.dev.tsv\")\n",
        "    queries_eval = open_file(\"data/queries.eval.tsv\")\n",
        "\n",
        "    # Load qRels train & dev\n",
        "    # These only have relevant files i.e binary relevance judgment score\n",
        "    # will be 1 for all the entries\n",
        "    qrels_train = open_file(\"data/qrels.train.tsv\", keys = [0, 2])\n",
        "    qrels_dev = open_file(\"data/qrels.dev.tsv\", keys = [0, 2])\n",
        "\n",
        "    return passages, queries_train, queries_dev, queries_eval, qrels_train, qrels_dev"
      ],
      "metadata": {
        "id": "RpxJZ9wGyKek"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "passages, queries_train, queries_dev, queries_eval, qrels_train, qrels_dev = load_and_preprocess_dataset()"
      ],
      "metadata": {
        "id": "HzO5njB46uFi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_samples(file_name, dict_, n = 2):\n",
        "    print('-' * 15)\n",
        "    print(f'Samples for {file_name}:')\n",
        "    samples = list(dict_.items())[:2]\n",
        "    for entry in samples:\n",
        "        print(entry)\n",
        "\n",
        "print_samples(\"passages\", passages)\n",
        "print_samples(\"queries_train\", queries_train)\n",
        "print_samples(\"queries_dev\", queries_dev)\n",
        "print_samples(\"queries_eval\", queries_eval)\n",
        "print_samples(\"qrels_train\", qrels_train)\n",
        "print_samples(\"qrels_dev\", qrels_dev)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpvNh0BW60C9",
        "outputId": "6cbc0318-749d-4941-91a4-22ff67105d4d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------\n",
            "Samples for passages:\n",
            "('0', 'The presence of communication amid scientific minds was equally important to the success of the Manhattan Project as scientific intellect was. The only cloud hanging over the impressive achievement of the atomic researchers and engineers is what their success truly meant; hundreds of thousands of innocent lives obliterated.')\n",
            "('1', 'The Manhattan Project and its atomic bomb helped bring an end to World War II. Its legacy of peaceful uses of atomic energy continues to have an impact on history and science.')\n",
            "---------------\n",
            "Samples for queries_train:\n",
            "('121352', 'define extreme')\n",
            "('634306', 'what does chattel mean on credit history')\n",
            "---------------\n",
            "Samples for queries_dev:\n",
            "('1048578', 'cost of endless pools/swim spa')\n",
            "('1048579', 'what is pcnt')\n",
            "---------------\n",
            "Samples for queries_eval:\n",
            "('786436', 'what is prescribed to treat thyroid storm')\n",
            "('9', ' Refer to the data. Diminishing returns begin to occur with the hiring of the _________ unit of labor ')\n",
            "---------------\n",
            "Samples for qrels_train:\n",
            "('1185869', '0')\n",
            "('1185868', '16')\n",
            "---------------\n",
            "Samples for qrels_dev:\n",
            "('1102432', '2026790')\n",
            "('1102431', '7066867')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Model"
      ],
      "metadata": {
        "id": "hl9XrFCNzNOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install torch ir_datasets faiss-cpu wandb numpy scikit-learn sentence-transformers transformers tqdm scipy matplotlib rank-eval faiss-gpu-cu11"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BumN0zDx1-9n",
        "outputId": "50bde121-40ae-41c7-fe52-e2c1f829ff05"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting ir_datasets\n",
            "  Using cached ir_datasets-0.5.10-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting faiss-cpu\n",
            "  Using cached faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.14.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Collecting rank-eval\n",
            "  Downloading rank_eval-0.1.3-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from ir_datasets) (4.13.3)\n",
            "Collecting inscriptis>=2.2.0 (from ir_datasets)\n",
            "  Downloading inscriptis-2.6.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.11/dist-packages (from ir_datasets) (5.3.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ir_datasets) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from ir_datasets) (2.32.3)\n",
            "Collecting trec-car-tools>=2.5.4 (from ir_datasets)\n",
            "  Downloading trec_car_tools-2.6-py3-none-any.whl.metadata (640 bytes)\n",
            "Collecting lz4>=3.1.10 (from ir_datasets)\n",
            "  Downloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting warc3-wet>=0.2.3 (from ir_datasets)\n",
            "  Downloading warc3_wet-0.2.5-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting warc3-wet-clueweb09>=0.2.5 (from ir_datasets)\n",
            "  Downloading warc3-wet-clueweb09-0.2.5.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting zlib-state>=0.1.3 (from ir_datasets)\n",
            "  Downloading zlib_state-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting ijson>=3.1.3 (from ir_datasets)\n",
            "  Downloading ijson-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting unlzw3>=0.2.1 (from ir_datasets)\n",
            "  Downloading unlzw3-0.2.3-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: pyarrow>=16.1.0 in /usr/local/lib/python3.11/dist-packages (from ir_datasets) (18.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.2)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.25.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: numba>=0.54.1 in /usr/local/lib/python3.11/dist-packages (from rank-eval) (0.60.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from rank-eval) (2.2.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from rank-eval) (0.9.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->ir_datasets) (2.6)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54.1->rank-eval) (0.43.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ir_datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ir_datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ir_datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ir_datasets) (2025.1.31)\n",
            "Collecting cbor>=1.0.0 (from trec-car-tools>=2.5.4->ir_datasets)\n",
            "  Downloading cbor-1.0.0.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->rank-eval) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->rank-eval) (2025.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m108.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ir_datasets-0.5.10-py3-none-any.whl (859 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m859.0/859.0 kB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rank_eval-0.1.3-py3-none-any.whl (17 kB)\n",
            "Downloading ijson-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.2/119.2 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading inscriptis-2.6.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trec_car_tools-2.6-py3-none-any.whl (8.4 kB)\n",
            "Downloading unlzw3-0.2.3-py3-none-any.whl (6.7 kB)\n",
            "Downloading warc3_wet-0.2.5-py3-none-any.whl (18 kB)\n",
            "Downloading zlib_state-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
            "Building wheels for collected packages: warc3-wet-clueweb09, cbor\n",
            "  Building wheel for warc3-wet-clueweb09 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for warc3-wet-clueweb09: filename=warc3_wet_clueweb09-0.2.5-py3-none-any.whl size=18919 sha256=2505d9c55827f81f3bf316de7144ef2f997eef327042e9945c92027159302f88\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/f9/dc/2dd16d3330e327236e4d407941975c42d5159d200cdb7922d8\n",
            "  Building wheel for cbor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cbor: filename=cbor-1.0.0-cp311-cp311-linux_x86_64.whl size=53931 sha256=0bfa191930f8c8c29592ecb765b563c38085f2dd72858c1b0863b001997c3809\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/6b/45/0c34253b1af07d1d9dc524f6d44d74a6b191c43152e6aaf641\n",
            "Successfully built warc3-wet-clueweb09 cbor\n",
            "Installing collected packages: warc3-wet-clueweb09, warc3-wet, ijson, cbor, zlib-state, unlzw3, trec-car-tools, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lz4, faiss-cpu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, inscriptis, rank-eval, nvidia-cusolver-cu12, ir_datasets\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed cbor-1.0.0 faiss-cpu-1.10.0 ijson-3.3.0 inscriptis-2.6.0 ir_datasets-0.5.10 lz4-4.4.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 rank-eval-0.1.3 trec-car-tools-2.6 unlzw3-0.2.3 warc3-wet-0.2.5 warc3-wet-clueweb09-0.2.5 zlib-state-0.1.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "import ir_datasets\n",
        "import faiss\n",
        "import wandb\n",
        "import heapq\n",
        "import time\n",
        "import sys\n",
        "import random\n",
        "import string\n",
        "import os\n",
        "import pickle\n",
        "import math\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import ndcg_score, recall_score\n",
        "from collections import defaultdict\n",
        "from scipy.sparse import csr_matrix\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from rank_eval import Qrels, Run, evaluate"
      ],
      "metadata": {
        "id": "J7DVSg84zPWE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('facebook/contriever-msmarco')\n",
        "model = AutoModel.from_pretrained('facebook/contriever-msmarco').to(device)\n",
        "\n",
        "# Mean pooling\n",
        "def mean_pooling(token_embeddings, mask):\n",
        "    token_embeddings = token_embeddings.masked_fill(~mask[..., None].bool(), 0.)\n",
        "    sentence_embeddings = token_embeddings.sum(dim=1) / mask.sum(dim=1)[..., None]\n",
        "    return sentence_embeddings"
      ],
      "metadata": {
        "id": "GU3Qqiap6uS0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model\n",
        "def evaluate_model(model, passages, dev_queries, dev_qrels, tokenizer, device, batch_size = 32, batch_size_inference = 128, min_weight = 1e-5, query_topk = 128, doc_topk = 1024, quick_mode = True):\n",
        "    base_model = model.module if isinstance(model, nn.DataParallel) else model\n",
        "\n",
        "    model.eval()\n",
        "    all_results = {}  # Store results for each query\n",
        "\n",
        "    # Filter passages if in quick mode\n",
        "    if quick_mode:\n",
        "        relevant_passage_ids = set()\n",
        "        for qid in dev_qrels:\n",
        "            relevant_passage_ids.update(dev_qrels[qid])\n",
        "        passages = {passage_id: passages[passage_id] for passage_id in relevant_passage_ids}\n",
        "        print(f\"Quick mode: using {len(passages)} passages for evaluation\")\n",
        "\n",
        "    # Filter queries if in quick mode\n",
        "    if quick_mode:\n",
        "        dev_queries = dict(list(dev_queries.items())[:200])\n",
        "        dev_qrels = {qid: dev_qrels[qid] for qid in dev_queries if qid in dev_qrels}\n",
        "        print(f\"Quick mode: using {len(dev_queries)} queries for evaluation\")\n",
        "\n",
        "    # Dense retrieval using FAISS\n",
        "    print(\"Building FAISS index...\")\n",
        "    passage_ids = list(passages.keys())\n",
        "    passage_embeddings = []\n",
        "\n",
        "    # Encode all passages\n",
        "    for i in tqdm(range(0, len(passage_ids), batch_size_inference), desc = \"Encoding passages\"):\n",
        "        batch_passages = [passages[pid] for pid in passage_ids[i:i + batch_size_inference]]\n",
        "\n",
        "        # Pad till the model's configured max_len (512)\n",
        "        passage_inputs = tokenizer(batch_passages, padding = True, truncation = True, return_tensors = 'pt')\n",
        "        passage_inputs = {k: v.to(device) for k, v in passage_inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(passage_inputs[\"input_ids\"], passage_inputs[\"attention_mask\"])\n",
        "            batch_embeddings = mean_pooling(outputs[0], passage_inputs['attention_mask'])\n",
        "            passage_embeddings.append(batch_embeddings.cpu().numpy())\n",
        "\n",
        "    passage_embeddings = np.vstack(passage_embeddings)\n",
        "\n",
        "    # Build FAISS index\n",
        "    dimension = passage_embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(dimension)\n",
        "    index.add(passage_embeddings)\n",
        "\n",
        "    # Evaluate queries\n",
        "    with torch.no_grad():\n",
        "        for qid, query in tqdm(dev_queries.items(), desc = \"Evaluating\"):\n",
        "            if qid not in dev_qrels:\n",
        "                continue\n",
        "\n",
        "            # Pad till the model's configured max_len (512)\n",
        "            query_input = tokenizer(query, padding = True, truncation = True, return_tensors = 'pt')\n",
        "            query_input = {k: v.to(device) for k, v in query_input.items()}\n",
        "            output = model(query_input[\"input_ids\"], query_input[\"attention_mask\"])\n",
        "            query_embedding = mean_pooling(output[0], query_input['attention_mask'])\n",
        "            query_embedding = query_embedding.cpu().numpy()\n",
        "\n",
        "            # Search using FAISS\n",
        "            scores, indices = index.search(query_embedding, 1000)\n",
        "            search_results = [(passage_ids[idx], float(score)) for idx, score in zip(indices[0], scores[0])]\n",
        "            all_results[qid] = search_results\n",
        "\n",
        "    # Create rank_eval Run and Qrels objects\n",
        "    run = Run()\n",
        "    qrels = Qrels()\n",
        "\n",
        "    # Add results to Run object\n",
        "    for qid in all_results:\n",
        "        passage_ids = [str(passage_id) for passage_id, score in all_results[qid]]\n",
        "        scores = [float(score) for _, score in all_results[qid]]\n",
        "        run.add(qid, passage_ids, scores)\n",
        "\n",
        "    # Add relevance judgments to Qrels object\n",
        "    for qid in dev_qrels:\n",
        "        qrels.add(qid, [str(doc_id) for doc_id in dev_qrels[qid]], [1] * len(dev_qrels[qid]))\n",
        "\n",
        "    # Evaluate using rank_eval\n",
        "    metrics = [\"ndcg@10\", \"ndcg@100\", \"ndcg@1000\", \"recall@10\", \"recall@100\", \"recall@1000\", \"mrr@10\"]\n",
        "    results = evaluate(qrels, run, metrics)\n",
        "\n",
        "    return (\n",
        "        results[\"mrr@10\"],\n",
        "        {\n",
        "            '10': results[\"ndcg@10\"],\n",
        "            '100': results[\"ndcg@100\"],\n",
        "            '1000': results[\"ndcg@1000\"]\n",
        "        },\n",
        "        {\n",
        "            '10': results[\"recall@10\"],\n",
        "            '100': results[\"recall@100\"],\n",
        "            '1000': results[\"recall@1000\"]\n",
        "        }\n",
        "    )\n"
      ],
      "metadata": {
        "id": "KeB_k9CVyge2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mrr_10, ndcg, recall = evaluate_model(model = model, passages = passages, dev_queries = queries_dev, dev_qrels = qrels_dev, tokenizer = tokenizer, device = device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnIGPmWT4wvw",
        "outputId": "ecd8032f-60ff-4b41-ff55-2e46004ac068"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quick mode: using 10 passages for evaluation\n",
            "Quick mode: using 200 queries for evaluation\n",
            "Building FAISS index...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding passages: 100%|██████████| 1/1 [00:00<00:00,  1.13it/s]\n",
            "Evaluating: 100%|██████████| 200/200 [00:01<00:00, 124.09it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/rank_eval/metrics.py:222: NumbaTypeSafetyWarning: unsafe cast from uint64 to int64. Precision may be lost.\n",
            "  scores[i] = _ndcg(qrels[i], run[i], k, jarvelin)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics\n",
        "print(\"Final Evaluation\")\n",
        "print(f\"MRR@10: {mrr_10:.4f}\")\n",
        "print(f\"nDCG@10: {ndcg['10']:.4f}, nDCG@100: {ndcg['100']:.4f}, nDCG@1000: {ndcg['1000']:.4f}\")\n",
        "print(f\"Recall@10: {recall['10']:.4f}, Recall@100: {recall['100']:.4f}, Recall@1000: {recall['1000']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8Ku0xkK5LEe",
        "outputId": "128c20d5-824f-418b-a010-4ab371afe980"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Evaluation\n",
            "MRR@10: 0.7461\n",
            "nDCG@10: 0.7981, nDCG@100: 0.7981, nDCG@1000: 0.7981\n",
            "Recall@10: 1.0000, Recall@100: 1.0000, Recall@1000: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vejZ88C97mtz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}