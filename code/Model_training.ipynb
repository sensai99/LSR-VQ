{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGdmtsKax9eY",
        "outputId": "ec2bb2e8-f593-4501-c43f-67137134889c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.30.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\n",
            "Requirement already satisfied: rank-eval in /usr/local/lib/python3.11/dist-packages (0.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rank-eval) (2.0.2)\n",
            "Requirement already satisfied: numba>=0.54.1 in /usr/local/lib/python3.11/dist-packages (from rank-eval) (0.60.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from rank-eval) (2.2.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from rank-eval) (0.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from rank-eval) (4.67.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54.1->rank-eval) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->rank-eval) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->rank-eval) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->rank-eval) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->rank-eval) (1.17.0)\n",
            "Requirement already satisfied: ranx in /usr/local/lib/python3.11/dist-packages (0.3.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ranx) (2.0.2)\n",
            "Requirement already satisfied: numba>=0.54.1 in /usr/local/lib/python3.11/dist-packages (from ranx) (0.60.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from ranx) (2.2.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from ranx) (0.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from ranx) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ranx) (1.15.2)\n",
            "Requirement already satisfied: ir-datasets in /usr/local/lib/python3.11/dist-packages (from ranx) (0.5.10)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from ranx) (13.9.4)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.11/dist-packages (from ranx) (3.10.16)\n",
            "Requirement already satisfied: lz4 in /usr/local/lib/python3.11/dist-packages (from ranx) (4.4.4)\n",
            "Requirement already satisfied: cbor2 in /usr/local/lib/python3.11/dist-packages (from ranx) (5.6.5)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from ranx) (0.13.2)\n",
            "Requirement already satisfied: fastparquet in /usr/local/lib/python3.11/dist-packages (from ranx) (2024.11.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54.1->ranx) (0.43.0)\n",
            "Requirement already satisfied: cramjam>=2.3 in /usr/local/lib/python3.11/dist-packages (from fastparquet->ranx) (2.10.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from fastparquet->ranx) (2025.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from fastparquet->ranx) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->ranx) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->ranx) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->ranx) (2025.2)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from ir-datasets->ranx) (4.13.4)\n",
            "Requirement already satisfied: inscriptis>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from ir-datasets->ranx) (2.6.0)\n",
            "Requirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.11/dist-packages (from ir-datasets->ranx) (5.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ir-datasets->ranx) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from ir-datasets->ranx) (2.32.3)\n",
            "Requirement already satisfied: trec-car-tools>=2.5.4 in /usr/local/lib/python3.11/dist-packages (from ir-datasets->ranx) (2.6)\n",
            "Requirement already satisfied: warc3-wet>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from ir-datasets->ranx) (0.2.5)\n",
            "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /usr/local/lib/python3.11/dist-packages (from ir-datasets->ranx) (0.2.5)\n",
            "Requirement already satisfied: zlib-state>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from ir-datasets->ranx) (0.1.9)\n",
            "Requirement already satisfied: ijson>=3.1.3 in /usr/local/lib/python3.11/dist-packages (from ir-datasets->ranx) (3.3.0)\n",
            "Requirement already satisfied: unlzw3>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from ir-datasets->ranx) (0.2.3)\n",
            "Requirement already satisfied: pyarrow>=16.1.0 in /usr/local/lib/python3.11/dist-packages (from ir-datasets->ranx) (18.1.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->ranx) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->ranx) (2.19.1)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn->ranx) (3.10.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets->ranx) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets->ranx) (4.13.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->ranx) (0.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->ranx) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->ranx) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->ranx) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->ranx) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->ranx) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->ranx) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->ranx) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ir-datasets->ranx) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ir-datasets->ranx) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ir-datasets->ranx) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->ir-datasets->ranx) (2025.1.31)\n",
            "Requirement already satisfied: cbor>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from trec-car-tools>=2.5.4->ir-datasets->ranx) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install --upgrade accelerate\n",
        "!pip install rank-eval\n",
        "!pip install ranx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MHwP1evIr23",
        "outputId": "33cdf8cd-9fc2-480d-8db0-9c64cf723de6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zEiw7MqrBjNY"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm.autonotebook import trange\n",
        "import numpy as np\n",
        "import json\n",
        "import random\n",
        "from torch.utils.data import DataLoader, RandomSampler\n",
        "from tqdm import tqdm\n",
        "from ranx import Qrels, Run, evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc"
      ],
      "metadata": {
        "id": "DkcWQJmZboDo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CMzdjmmMZQ9h"
      },
      "outputs": [],
      "source": [
        "from utils import tsv_to_dict_multiple, tsv_to_dict_unqiue"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = None\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "SjgiN98MmAy7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQV6v81uEHuK",
        "outputId": "d4a50062-cf8b-43e7-c5e5-3fa913798a75"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Apr 30 19:43:13 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0             49W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWGZx53JNcJR"
      },
      "source": [
        "### Load raw training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3Z3-ke6Tfp7",
        "outputId": "5f0fdb6b-566b-4544-b9d5-7336aa096640"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# base-model\n",
        "# model_name = 'facebook/contriever'\n",
        "# fine-tuned MS-MARCO\n",
        "model_name = \"facebook/contriever-msmarco\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Fh8HbnuzZQ9i"
      },
      "outputs": [],
      "source": [
        "passages = tsv_to_dict_unqiue(os.path.join('/content/drive/MyDrive/685/data', \"collection.tsv\"))\n",
        "queries = tsv_to_dict_unqiue(os.path.join('/content/drive/MyDrive/685/data', \"queries.train.tsv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zEhi3tF8T3FL"
      },
      "outputs": [],
      "source": [
        "def create_dataset(address, n_negatives=2,cache_dir='hf',sample_count=1000):\n",
        "    def train_gen():\n",
        "      with open(address,'r') as f:\n",
        "          count = 0\n",
        "          for line in f:\n",
        "              count += 1\n",
        "              if sample_count != None and count >= sample_count: return\n",
        "              data_sample = json.loads(line)\n",
        "              negatives = []\n",
        "              for neg_id in random.sample(data_sample['neg']['bm25'],n_negatives): negatives.append(passages[str(neg_id)])\n",
        "              yield {'query': queries[str(data_sample['qid'])],'positive':passages[str(random.sample(data_sample['pos'],1)[0])],'negatives':negatives}\n",
        "    return Dataset.from_generator(train_gen, cache_dir = cache_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pPuwxVRQT8o4"
      },
      "outputs": [],
      "source": [
        "dataset = create_dataset(os.path.join('/content/drive/MyDrive/685/data','msmarco-hard-negatives-bm25_1k.jsonl'), 2, 'hf',2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dqTDgBxkPNyr"
      },
      "outputs": [],
      "source": [
        "split_dataset = dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "# There might be a split over\n",
        "train_dataset = split_dataset['train']\n",
        "eval_dataset = split_dataset['test']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32NhWmxaYSvK"
      },
      "source": [
        "### Custom data collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "77DZ0UU7YZwH"
      },
      "outputs": [],
      "source": [
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "class CustomDataCollatorWithPadding(DataCollatorWithPadding):\n",
        "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
        "        query_texts = []\n",
        "        pos_texts = []\n",
        "        neg_texts = []\n",
        "\n",
        "        for feature in features:\n",
        "            query_texts.append(feature['query'])\n",
        "            pos_texts.append(feature['positive'])\n",
        "            if 'negatives' in feature.keys():\n",
        "                for neg_text in feature['negatives']: neg_texts.append(neg_text)\n",
        "\n",
        "        tokenized_query_texts = self.tokenizer(\n",
        "                query_texts,\n",
        "                max_length=self.max_length,\n",
        "                padding=self.padding,\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\",\n",
        "                add_special_tokens=True)\n",
        "\n",
        "        tokenized_pos_texts = self.tokenizer(\n",
        "                pos_texts,\n",
        "                max_length=self.max_length,\n",
        "                padding=self.padding,\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\",\n",
        "                add_special_tokens=True)\n",
        "\n",
        "        tokenized_texts = {\n",
        "            'tokenized_queries' : tokenized_query_texts,\n",
        "            'tokenized_positives' : tokenized_pos_texts,\n",
        "        }\n",
        "\n",
        "\n",
        "        if len(neg_texts) > 0:\n",
        "            tokenized_neg_texts = self.tokenizer(\n",
        "                  neg_texts,\n",
        "                  max_length=self.max_length,\n",
        "                  padding=self.padding,\n",
        "                  truncation=True,\n",
        "                  return_tensors=\"pt\",\n",
        "                  add_special_tokens=True)\n",
        "            tokenized_texts['tokenized_negatives'] = tokenized_neg_texts\n",
        "\n",
        "        return tokenized_texts\n",
        "\n",
        "custom_data_collator = CustomDataCollatorWithPadding(\n",
        "    tokenizer=tokenizer,\n",
        "    padding='longest',\n",
        "    max_length=tokenizer.model_max_length\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjQ1TYw2Pti-",
        "outputId": "494af11f-f978-4b8c-d9e8-7d055aeaefa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 30])\n",
            "torch.Size([8, 153])\n",
            "torch.Size([16, 176])\n"
          ]
        }
      ],
      "source": [
        "out_train = custom_data_collator([train_dataset[i] for i in range(8)])\n",
        "print(out_train['tokenized_queries']['input_ids'].shape)\n",
        "print(out_train['tokenized_positives']['input_ids'].shape)\n",
        "print(out_train['tokenized_negatives']['input_ids'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwoMdR_xNXBT",
        "outputId": "b6ba7179-a3cf-474c-842d-d9a330347900"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 11])\n",
            "torch.Size([2, 160])\n"
          ]
        }
      ],
      "source": [
        "out_eval = custom_data_collator([eval_dataset[i] for i in range(2)])\n",
        "print(out_eval['tokenized_queries']['input_ids'].shape)\n",
        "print(out_eval['tokenized_positives']['input_ids'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEC5DkMBZctk"
      },
      "source": [
        "### Contrastive loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gfLPDDjdZqZW"
      },
      "outputs": [],
      "source": [
        "#enter your code here\n",
        "def get_contriever_emb(model_output, attention_mask):\n",
        "    last_hidden = model_output[\"last_hidden_state\"]\n",
        "    last_hidden = last_hidden.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
        "    emb = last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
        "    return emb\n",
        "\n",
        "def contrastive_loss(query_embs,pos_embs,neg_embs):\n",
        "    temp = 0.05\n",
        "    query_embs = query_embs/temp\n",
        "    similarities_pos = torch.mm(query_embs,torch.transpose(pos_embs,0,1))\n",
        "    similarities = similarities_pos\n",
        "    if neg_embs != None:\n",
        "        similarities_neg = torch.mm(query_embs,torch.transpose(neg_embs,0,1))\n",
        "        similarities = torch.cat((similarities_pos,similarities_neg),dim=1)\n",
        "    return F.cross_entropy(similarities,torch.arange(0,query_embs.shape[0]).to('cuda'))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12zUzO2vZt-g"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_hXrtCLcZQ9k"
      },
      "outputs": [],
      "source": [
        "class Quantize(nn.Module):\n",
        "    def __init__(self, codebook_vector_dim, num_clusters, decay = 0.99, eps = 1e-5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dim = codebook_vector_dim\n",
        "        self.num_clusters = num_clusters\n",
        "        self.decay = decay\n",
        "        self.eps = eps\n",
        "\n",
        "        embed = torch.randn(codebook_vector_dim, num_clusters)\n",
        "        self.register_buffer(\"embed\", embed)\n",
        "        self.register_buffer(\"cluster_size\", torch.zeros(num_clusters))\n",
        "        self.register_buffer(\"embed_avg\", embed.clone())\n",
        "\n",
        "    def forward(self, input, training = True):\n",
        "        input = torch.nn.functional.normalize(input, p=2, dim=1)\n",
        "        flatten = input.reshape(-1, self.dim)\n",
        "        dist = (\n",
        "            flatten.pow(2).sum(1, keepdim = True)\n",
        "            - 2 * flatten @ self.embed\n",
        "            + self.embed.pow(2).sum(0, keepdim = True)\n",
        "        )\n",
        "        _, embed_ind = (-dist).max(1)\n",
        "        embed_onehot = F.one_hot(embed_ind, self.num_clusters).type(flatten.dtype)\n",
        "        embed_ind = embed_ind.view(*input.shape[:-1])\n",
        "        quantize = self.embed_code(embed_ind)\n",
        "\n",
        "        if training:\n",
        "            embed_onehot_sum = embed_onehot.sum(0)\n",
        "            embed_sum = flatten.transpose(0, 1) @ embed_onehot\n",
        "\n",
        "            self.cluster_size.data.mul_(self.decay).add_(\n",
        "                embed_onehot_sum, alpha=1 - self.decay\n",
        "            )\n",
        "            self.embed_avg.data.mul_(self.decay).add_(embed_sum, alpha=1 - self.decay)\n",
        "            n = self.cluster_size.sum()\n",
        "            cluster_size = (\n",
        "                (self.cluster_size + self.eps) / (n + self.num_clusters * self.eps) * n\n",
        "            )\n",
        "            embed_normalized = self.embed_avg / cluster_size.unsqueeze(0)\n",
        "            self.embed.data.copy_(embed_normalized)\n",
        "\n",
        "        quantize = input + (quantize - input).detach()\n",
        "        return quantize, embed_ind\n",
        "\n",
        "    def embed_code(self, embed_id):\n",
        "        return F.embedding(embed_id, self.embed.transpose(0, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "uEaFY2mFZQ9k"
      },
      "outputs": [],
      "source": [
        "class Sparse_Layer(nn.Module):\n",
        "    def __init__(self,d,D):\n",
        "        super().__init__()\n",
        "\n",
        "        self.up_projection = nn.Linear(d,D)\n",
        "\n",
        "    def forward(self,embeds):\n",
        "        print(\"NaNs before matmul:\", torch.isnan(embeds).any())\n",
        "        S = self.up_projection(embeds)\n",
        "        print(\"NaNs after projection:\", torch.isnan(S).any())\n",
        "        S = torch.log(1 + torch.relu(S))\n",
        "        print(\"NaNs after logrelu:\", torch.isnan(S).any())\n",
        "        S = torch.clamp(S, max=10)\n",
        "        print(\"NaNs after clamp:\", torch.isnan(S).any())\n",
        "        S = torch.nn.functional.normalize(S, p=2, dim=1,eps=1e-12)\n",
        "        print(\"NaNs after normalize:\", torch.isnan(S).any())\n",
        "        return S"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "W-uFO5COZQ9k"
      },
      "outputs": [],
      "source": [
        "class LSR_VQ(nn.Module):\n",
        "    def __init__(self, model_name, emb_dim, num_clusters, d, D, freeze_contriver=False):\n",
        "        super().__init__()\n",
        "        self.model = AutoModel.from_pretrained(model_name,device_map='auto')\n",
        "        if freeze_contriver:\n",
        "            for param in self.model.parameters():\n",
        "                param.requires_grad = False\n",
        "        self.quantizer = Quantize(emb_dim,num_clusters)\n",
        "        self.sparse_layer = Sparse_Layer(d,D)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        query_embs = self.model(**inputs['tokenized_queries'])\n",
        "        query_dense = get_contriever_emb(query_embs,inputs['tokenized_queries']['attention_mask'])\n",
        "\n",
        "        pos_embs = self.model(**inputs['tokenized_positives'])\n",
        "        pos_dense = get_contriever_emb(pos_embs,inputs['tokenized_positives']['attention_mask'])\n",
        "\n",
        "        neg_embs = self.model(**inputs['tokenized_negatives'])\n",
        "        neg_dense = get_contriever_emb(neg_embs,inputs['tokenized_negatives']['attention_mask'])\n",
        "\n",
        "        quantized_embs, _ = self.quantizer(torch.cat([query_dense,pos_dense,neg_dense],dim=0),True)\n",
        "\n",
        "        query_offset = 0\n",
        "        pos_offset = query_dense.shape[0]\n",
        "        neg_offset = query_dense.shape[0] + pos_dense.shape[0]\n",
        "\n",
        "        query_quantized_embs = quantized_embs[:pos_offset]\n",
        "        pos_quantized_embs = quantized_embs[pos_offset:neg_offset]\n",
        "        neg_quantized_embs = quantized_embs[neg_offset:]\n",
        "\n",
        "        query_sparse = self.sparse_layer(query_quantized_embs)\n",
        "        pos_sparse = self.sparse_layer(pos_quantized_embs)\n",
        "        neg_sparse = self.sparse_layer(neg_quantized_embs)\n",
        "\n",
        "        return query_dense,query_sparse,pos_dense,pos_sparse,neg_dense,neg_sparse\n",
        "\n",
        "    def get_sparse_rep(self,input_ids,attention_mask):\n",
        "        embs = self.model(input_ids)\n",
        "        dense_embs = get_contriever_emb(embs,attention_mask)\n",
        "\n",
        "        quantized_embs, codes = self.quantize(dense_embs,False)\n",
        "\n",
        "        sparse_rep = self.sparse_layer(quantized_embs)\n",
        "\n",
        "        return sparse_rep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "iQdebk-1ZQ9k"
      },
      "outputs": [],
      "source": [
        "model = LSR_VQ(model_name,768,2048,768,8192)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVGQZfiXZQ9k"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "OO9ltyZLZ3yI"
      },
      "outputs": [],
      "source": [
        "#enter your code here to define trainer\n",
        "class LSRVQ_Trainer(Trainer):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.query_reg_lambda = 0.1\n",
        "        self.passage_reg_lambda = 0.1\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False,num_items_in_batch=None):\n",
        "\n",
        "        query_dense, query_sparse, pos_dense, pos_sparse, neg_dense, neg_sparse = self.model(inputs)\n",
        "\n",
        "        # Constrastive loss\n",
        "        c_loss = contrastive_loss(query_dense,pos_dense,neg_dense)\n",
        "\n",
        "        print('c_loss',c_loss)\n",
        "\n",
        "        # Query sparsity loss\n",
        "        query_reg_loss = torch.sum(torch.mean(torch.abs(query_sparse), dim=0) ** 2)\n",
        "\n",
        "        print('q_loss',query_reg_loss)\n",
        "\n",
        "        # Document sparsity loss\n",
        "        passage_sparse = torch.cat([pos_sparse,neg_sparse], dim=0)\n",
        "        passage_reg_loss = torch.sum(torch.mean(torch.abs(passage_sparse), dim=0) ** 2)\n",
        "\n",
        "        print('p_loss',passage_reg_loss)\n",
        "\n",
        "        loss = c_loss + self.query_reg_lambda*query_reg_loss + self.passage_reg_lambda*passage_reg_loss\n",
        "\n",
        "        return (loss,torch.zeros(1)) if return_outputs else loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "6Fveksc6Zwr8"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./weights\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=0,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100,\n",
        "    eval_steps=5,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_total_limit=36,\n",
        "    save_steps=5,\n",
        "    load_best_model_at_end=True,\n",
        "    greater_is_better=False,\n",
        "    learning_rate=1e-4,\n",
        "    lr_scheduler_type = \"linear\",\n",
        "    fp16=True,\n",
        "    report_to='none',\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    remove_unused_columns=False,\n",
        "    # optim = \"adamw_8bit\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "s3MtVX_Dq2Wy"
      },
      "outputs": [],
      "source": [
        "trainer = LSRVQ_Trainer(\n",
        "        model = model,\n",
        "        args = training_args,\n",
        "        data_collator = custom_data_collator,\n",
        "        train_dataset = train_dataset,\n",
        "        eval_dataset = eval_dataset)\n",
        "\n",
        "trainer.can_return_loss=True\n",
        "\n",
        "def custom_eval_dataloader(eval_set):\n",
        "    return DataLoader(\n",
        "    eval_set,\n",
        "    batch_size=training_args.eval_batch_size,\n",
        "    sampler=RandomSampler(eval_dataset),\n",
        "    collate_fn=trainer.data_collator,\n",
        ")\n",
        "\n",
        "trainer.get_eval_dataloader = custom_eval_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKTck8JRZQ9l"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8Sndzj0Z-Qc"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "up5wVcoOZQ9l"
      },
      "outputs": [],
      "source": [
        "queries_dev = tsv_to_dict_unqiue(os.path.join('./data', \"queries.dev.small.tsv\"))\n",
        "qrels_dev = tsv_to_dict_multiple(os.path.join('./data', \"qrels.dev.small.tsv\"), keys = [0, 2])\n",
        "\n",
        "with open('content/drive/myDrive/685/embeddings/train/passage_ids.json') as f:\n",
        "    inference_passage_ids = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7QRTTCPfPlS"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "def encode_texts(model, tokenizer, text_ds, batch_size, verbose=False):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    text_ids = [i['_id'] for i in text_ds]\n",
        "    dataloader = DataLoader(text_ds, batch_size=batch_size, shuffle=False)\n",
        "    allemb = []\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader):\n",
        "            prepare_batch = [batch['text'][i] for i in range(len(batch['text']))]\n",
        "            tokenized_texts = tokenizer.batch_encode_plus(\n",
        "                prepare_batch,\n",
        "                max_length=tokenizer.model_max_length,\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\",\n",
        "                add_special_tokens=True,\n",
        "            ).to(device)\n",
        "            if verbose:\n",
        "                print(\"Batch size:\", len(batch['text']))\n",
        "                print(f\"Tokenized texts shape: {tokenized_texts['input_ids'].shape}\")\n",
        "                print(f\"Decode text sample {tokenizer.decode(tokenized_texts['input_ids'][0])}\")\n",
        "            emb = model.get_sparse_rep(**tokenized_texts,tokenized_texts['attention_mask'])\n",
        "            allemb.append(emb.cpu())\n",
        "    allemb = torch.cat(allemb, dim=0)\n",
        "    allemb = allemb.cpu().numpy()\n",
        "    return allemb, text_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ae_M0bgZQ9l"
      },
      "outputs": [],
      "source": [
        "corpus_ds = Dataset.from_dict({'_id':inference_passage_ids,'text':[passages[_id] for _id in inference_passage_ids]})\n",
        "queries_ds = Dataset.from_dict({'_id':list(queries_dev.keys()),'text':list(queries_dev.values())})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H-KuPc-ZQ9l"
      },
      "source": [
        "### Top-k Sparsification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taE_1AViZQ9l"
      },
      "outputs": [],
      "source": [
        "passage_topk = 1024\n",
        "\n",
        "passage_sparse_embeddings, passage_ids = encode_texts(model, tokenizer, corpus_ds, batch_size=64)\n",
        "\n",
        "# Apply top-k sparsification\n",
        "values, indices = torch.topk(passage_sparse_embeddings.abs(), k=passage_topk, dim=1)\n",
        "sparse_embeddings = torch.zeros_like(passage_sparse_embeddings)\n",
        "sparse_embeddings.scatter_(1, indices, passage_sparse_embeddings.gather(1, indices))\n",
        "\n",
        "passage_sparse_embeddings = sparse_embeddings.detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meX0DM3wZQ9l"
      },
      "source": [
        "### Inverted Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zS2uKxPbZQ9l"
      },
      "outputs": [],
      "source": [
        "inverted_index = defaultdict(list)\n",
        "min_weight = 1e-5\n",
        "for j, doc_id in enumerate(passage_ids):\n",
        "    doc_embedding = passage_sparse_embeddings[j]\n",
        "    # Get non-zero indices and their values\n",
        "    nonzero_indices = np.nonzero(doc_embedding)[0]  # Ensure indices are integers\n",
        "    for idx in nonzero_indices:\n",
        "        weight = doc_embedding[idx]\n",
        "        if weight > min_weight:\n",
        "            inverted_index[int(idx)].append((doc_id, float(weight)))\n",
        "\n",
        " # Sort postings lists by weight for each term\n",
        "for idx in inverted_index:\n",
        "    inverted_index[idx] = sorted(inverted_index[idx], key=lambda x: abs(x[1]), reverse=True)\n",
        "\n",
        "# Convert to more efficient data structure\n",
        "optimized_index = {\n",
        "    idx: (\n",
        "        np.array([doc_id for doc_id, _ in postings], dtype=np.int32),\n",
        "        np.array([weight for _, weight in postings], dtype=np.float32)\n",
        "    )\n",
        "    for idx, postings in inverted_index.items()\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEmfKme8ZQ9l"
      },
      "source": [
        "### Search Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OidiixihZQ9m"
      },
      "outputs": [],
      "source": [
        "def search_inverted_index(query_embedding, inverted_index, query_topk=128, min_weight=1e-5):\n",
        "    scores = defaultdict(float)\n",
        "    seen_docs = set()\n",
        "\n",
        "    # Get top-k query dimensions by weight\n",
        "    weights = [(idx, weight) for idx, weight in enumerate(query_embedding) if abs(weight) > min_weight]\n",
        "    top_weights = heapq.nlargest(query_topk, weights, key=lambda x: abs(x[1]))\n",
        "\n",
        "    # Process each query term\n",
        "    for idx, query_weight in top_weights:\n",
        "        if idx not in inverted_index:\n",
        "            continue\n",
        "\n",
        "        doc_ids, doc_weights = inverted_index[idx]\n",
        "\n",
        "        # Only process top documents per term\n",
        "        for doc_id, doc_weight in zip(doc_ids, doc_weights):\n",
        "            scores[doc_id] += query_weight * doc_weight\n",
        "            seen_docs.add(doc_id)\n",
        "\n",
        "    # Use numpy for final scoring\n",
        "    if seen_docs:\n",
        "        doc_ids = np.array(list(seen_docs))\n",
        "        doc_scores = np.array([scores[doc_id] for doc_id in doc_ids])\n",
        "\n",
        "        # Get top 1000 results efficiently\n",
        "        top_k = min(1000, len(doc_scores))\n",
        "        top_indices = np.argpartition(doc_scores, -top_k)[-top_k:]\n",
        "        top_indices = top_indices[np.argsort(-doc_scores[top_indices])]\n",
        "\n",
        "        return [(doc_ids[i], doc_scores[i]) for i in top_indices]\n",
        "\n",
        "    return []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHD9ghP6ZQ9m"
      },
      "source": [
        "### Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MijyO5f3ZQ9m"
      },
      "outputs": [],
      "source": [
        "query_sparse_embeddings, query_ids = encode_texts(model, tokenizer, queries_ds, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLLpoei3ZQ9m"
      },
      "outputs": [],
      "source": [
        "query_topk = 128\n",
        "\n",
        "all_results = {}\n",
        "\n",
        "for i,query_id in enumerate(query_ids):\n",
        "    query_embedding = query_sparse_embeddings[i].cpu().numpy()[0]\n",
        "    search_results = search_inverted_index(\n",
        "                query_embedding,\n",
        "                optimized_index,\n",
        "                query_topk=query_topk,\n",
        "                min_weight=min_weight\n",
        "            )\n",
        "    all_results[query_id] = search_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sHOkG_QZQ9m"
      },
      "source": [
        "### Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HG8-P7Q_ZQ9n"
      },
      "outputs": [],
      "source": [
        "# Create rank_eval Run and Qrels objects\n",
        "run = Run()\n",
        "qrels = Qrels()\n",
        "\n",
        "# Add results to Run object\n",
        "for qid in all_results:\n",
        "    doc_ids = [str(doc_id) for doc_id, score in all_results[qid]]\n",
        "    scores = [float(score) for _, score in all_results[qid]]\n",
        "    run.add(qid, doc_ids, scores)\n",
        "\n",
        "# Add relevance judgments to Qrels object\n",
        "for qid in qrels_dev:\n",
        "    qrels.add(qid, [str(doc_id) for doc_id in qrels_dev[qid]], [1] * len(qrels_dev[qid]))\n",
        "\n",
        "# Evaluate using rank_eval\n",
        "metrics = [\"ndcg@10\", \"ndcg@100\", \"ndcg@1000\", \"recall@10\", \"recall@100\", \"recall@1000\", \"mrr@10\"]\n",
        "results = evaluate(qrels, run, metrics)\n",
        "\n",
        "metrics = (\n",
        "    results[\"mrr@10\"],\n",
        "    {\n",
        "        '10': results[\"ndcg@10\"],\n",
        "        '100': results[\"ndcg@100\"],\n",
        "        '1000': results[\"ndcg@1000\"]\n",
        "    },\n",
        "    {\n",
        "        '10': results[\"recall@10\"],\n",
        "        '100': results[\"recall@100\"],\n",
        "        '1000': results[\"recall@1000\"]\n",
        "    }\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python [conda env:pytorchbase]",
      "language": "python",
      "name": "conda-env-pytorchbase-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}